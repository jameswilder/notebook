{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "\n",
    "encode_dims = 2\n",
    "enc_hidden = [1000,1000,encode_dims]\n",
    "dec_hidden = [1000,1000,784]\n",
    "disc_hidden = [1000,1000,1]\n",
    "#enc_hidden = [200,200,200,encode_dims]\n",
    "#dec_hidden = [200,200,200,784]\n",
    "#disc_hidden = [200,200,200,1]\n",
    "#enc_hidden = [100,encode_dims]\n",
    "#dec_hidden = [100,784]\n",
    "#disc_hidden = [100,1]\n",
    "\n",
    "def createHiddenLayers(layers, x, prefix, last_layer_relu):\n",
    "\ttrainables = []\n",
    "\tweights_and_biases = []\n",
    "\tnext_input_count = x.get_shape()[1].value\n",
    "\tinputs = x\n",
    "\tfor idx in range(len(layers)):\n",
    "\t\tprint idx\n",
    "\t\thidden_count = layers[idx]\n",
    "\t\tw_xh = tf.get_variable(prefix + 'w_' + repr(idx), [next_input_count, hidden_count],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer())\n",
    "#\t\tw_xh = tf.get_variable(prefix + 'w_' + repr(idx), [next_input_count, hidden_count],\n",
    "#\t\t\tinitializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\t\tif (idx != len(layers) - 1) or (prefix == 'dec_'):\n",
    "\t\t\tb_xh = tf.get_variable(prefix + 'b_' + repr(idx), initializer=tf.constant(0.01, shape=[hidden_count]))\n",
    "\t\telse:\n",
    "\t\t\tb_xh = tf.get_variable(prefix + 'b_' + repr(idx), initializer=tf.constant(0.0, shape=[hidden_count]))\n",
    " \t\ta_h = tf.add(tf.matmul(inputs, w_xh), b_xh)\n",
    "\t\tif last_layer_relu or (idx != len(layers) - 1):\n",
    "\t\t\tprint 'relu'\n",
    "\t\t\ty_h = tf.nn.relu(a_h)\n",
    "\t\telse:\n",
    "\t\t\tprint 'not relu'\n",
    "\t\t\ty_h = a_h\n",
    "\t\ttrainables.append(w_xh)\n",
    "\t\ttrainables.append(b_xh)\n",
    "\t\tprint w_xh.get_shape()\n",
    "\t\tprint b_xh.get_shape()\n",
    "\t\tweights_and_biases.append([w_xh, b_xh])\n",
    "\t\tnext_input_count = hidden_count\n",
    "\t\tinputs = y_h\n",
    "\treturn y_h, trainables, weights_and_biases\n",
    "\n",
    "def createNetworkFromWeightsAndBiases(x, weights_and_biases, last_layer_relu):\n",
    "\tinputs = x\n",
    "\tfor idx in range(len(weights_and_biases)):\n",
    "\t\tprint idx\n",
    "\t\ta_h = tf.add(tf.matmul(inputs, weights_and_biases[idx][0]), weights_and_biases[idx][1])\n",
    "\t\tif last_layer_relu or (idx != len(weights_and_biases) - 1):\n",
    "\t\t\tprint 'relu'\n",
    "\t\t\ty_h = tf.nn.relu(a_h)\n",
    "\t\telse:\n",
    "\t\t\tprint 'not relu'\n",
    "\t\t\ty_h = a_h\n",
    "\t\tinputs = y_h\n",
    "\treturn y_h\n",
    "\n",
    "print 'generator: encoder'\n",
    "gen_enc_x = tf.placeholder(tf.float32, [None, 784])\n",
    "gen_enc_y, gen_enc_trainables, gen_enc_weights_and_biases = createHiddenLayers(enc_hidden, gen_enc_x, \"enc_\", False)\n",
    "\n",
    "print 'generator: encoder'\n",
    "gen_enc_x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "gen_enc_y2 =createNetworkFromWeightsAndBiases(gen_enc_x2, gen_enc_weights_and_biases, False)\n",
    "\n",
    "print 'generator: decoder'\n",
    "gen_dec_y, gen_dec_trainables, gen_dec_weights_and_biases = createHiddenLayers(dec_hidden, gen_enc_y, \"dec_\", False)\n",
    "\n",
    "for x in range(len(gen_dec_weights_and_biases)):\n",
    "\tprint gen_dec_weights_and_biases[x][0].get_shape()\n",
    "\tprint gen_dec_weights_and_biases[x][1].get_shape()\n",
    "\n",
    "print 'generator: decoder only'\n",
    "gen_dec_x2 = tf.placeholder(tf.float32, [None, encode_dims])\n",
    "gen_dec_y2 = createNetworkFromWeightsAndBiases(gen_dec_x2, gen_dec_weights_and_biases, False)\n",
    "\n",
    "print 'discriminator - unlinked'\n",
    "disc_x = tf.placeholder(tf.float32, [None, encode_dims])\n",
    "disc_a, disc_trainables, disc_weights_and_baises = createHiddenLayers(disc_hidden, disc_x, \"disc_\", False)\n",
    "disc_y = tf.sigmoid(disc_a)\n",
    "\n",
    "print 'discriminator -linked to gen_enc_y2 (z)'\n",
    "disc_x2 = gen_enc_y2\n",
    "disc_a2 = createNetworkFromWeightsAndBiases(disc_x2, disc_weights_and_baises, False)\n",
    "disc_y2 = tf.sigmoid(disc_a2)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "z = tf.placeholder(tf.float32, [None, 2])\n",
    "z_ = tf.placeholder(tf.float32, [None, 2])\n",
    "disc_disc_y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "disc_gen_y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#gen_loss = tf.reduce_mean(\n",
    "#tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gen_dec_y))\n",
    "\n",
    "learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "gen_loss = tf.contrib.losses.mean_squared_error(gen_enc_x, gen_dec_y)\n",
    "#gen_train_step = tf.train.GradientDescentOptimizer(0.1).minimize(gen_loss)\n",
    "gen_train_step = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(gen_loss)\n",
    "\n",
    "#disc_disc_loss = tf.contrib.losses.log_loss(disc_y, disc_disc_y_)\n",
    "disc_disc_loss = tf.contrib.losses.mean_squared_error(disc_y, disc_disc_y_)\n",
    "#disc_disc_loss = tf.contrib.losses.absolute_difference(disc_y, disc_disc_y_)\n",
    "disc_disc_train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(disc_disc_loss, var_list=disc_trainables)\n",
    "#disc_disc_train_step = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(disc_disc_loss, var_list=disc_trainables)\n",
    "\n",
    "#disc_enc_loss = tf.contrib.losses.log_loss(disc_y2, disc_gen_y_)\n",
    "#disc_enc_loss = tf.contrib.losses.mean_squared_error(disc_y2, disc_gen_y_)\n",
    "disc_enc_loss = tf.contrib.losses.absolute_difference(disc_y2, disc_gen_y_)\n",
    "disc_enc_train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(disc_enc_loss, var_list=gen_enc_trainables)\n",
    "#disc_enc_train_step = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(disc_enc_loss, var_list=gen_enc_trainables)\n",
    "\n",
    "jpeg_data = tf.placeholder(tf.float32)\n",
    "get_constant_mul = tf.fill([28,28,1], 400.0)\n",
    "get_constant_max = tf.fill([28,28,1], 255.0)\n",
    "get_constant_min = tf.fill([28,28,1], 0.0)\n",
    "encode_jpeg = tf.image.encode_jpeg(tf.cast(tf.minimum(tf.maximum(tf.mul(tf.reshape(jpeg_data, [28,28,1]), get_constant_mul), get_constant_min), get_constant_max), tf.uint8), format='grayscale', quality=100)\n",
    "\n",
    "jpeg_data_grid = tf.placeholder(tf.float32)\n",
    "get_constant_mul_grid = tf.fill([28*21,28*21,1], 400.0)\n",
    "get_constant_max_grid = tf.fill([28*21,28*21,1], 255.0)\n",
    "get_constant_min_grid = tf.fill([28*21,28*21,1], 0.0)\n",
    "encode_jpeg_grid = tf.image.encode_jpeg(tf.cast(tf.minimum(tf.maximum(tf.mul(tf.reshape(jpeg_data_grid, [28*21,28*21,1]), get_constant_mul_grid), get_constant_min_grid), get_constant_max_grid), tf.uint8), format='grayscale', quality=100)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "fuzzy_ones_zeros = np.tile(np.array([0.95]), (batch_size,1))\n",
    "fuzzy_zeros_ones = np.tile(np.array([0.05]), (batch_size,1))\n",
    "ones_zeros = np.tile(np.array([1.0]), (batch_size,1))\n",
    "zeros_ones = np.tile(np.array([0.0]), (batch_size,1))\n",
    "print ones_zeros.shape\n",
    "print ones_zeros\n",
    "print zeros_ones.shape\n",
    "print zeros_ones\n",
    "\n",
    "#ones_zeros = np.append(np.ones((batch_size,1),dtype=np.float32), np.zeros((batch_size,1),dtype=np.float32), axis=1)\n",
    "#zeros_ones = np.append(np.zeros((batch_size,1),dtype=np.float32), np.ones((batch_size,1),dtype=np.float32), axis=1)\n",
    "disc_train_z_ = np.append(fuzzy_ones_zeros, fuzzy_zeros_ones, axis=0)\n",
    "print disc_train_z_.shape\n",
    "print disc_train_z_\n",
    "\n",
    "y1_values = []\n",
    "y2_values = []\n",
    "y3_values = []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# Pre-train discriminator\n",
    "#for i in range(1001):\n",
    "#\tbatch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "#\n",
    "#\tgen_loss_value, gen_enc_y_result = sess.run([gen_loss, gen_enc_y], feed_dict={gen_enc_x: batch_xs, y_: batch_ys})\n",
    "#\n",
    "#\tsampled_z = np.random.uniform(1.0, 1.0, size=(batch_size, encode_dims))\n",
    "##\tsampled_z2 = np.random.uniform(-2.0, -1.0, size=(batch_size, encode_dims))\n",
    "#\ttrain_z = np.append(sampled_z, gen_enc_y_result, axis=0)\n",
    "##\ttrain_z = np.append(sampled_z, sampled_z2, axis=0)\n",
    "##\tprint train_z\n",
    "\n",
    "#\t_, disc_disc_loss_value, disc_y_out = sess.run([disc_disc_train_step, disc_disc_loss, disc_y], feed_dict={disc_x: train_z, disc_disc_y_: disc_train_z_})\n",
    "#\t_, disc_enc_loss_value = sess.run([disc_enc_train_step, disc_enc_loss], feed_dict={gen_enc_x2: batch_xs, disc_gen_y_: ones_zeros})\n",
    "#\n",
    "#\tprint('{}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}'.format(i, gen_loss_value, disc_disc_loss_value, np.mean(gen_enc_y_result[0]), np.mean(gen_enc_y_result[1]), np.min(gen_enc_y_result[0]), np.max(gen_enc_y_result[0]), np.min(gen_enc_y_result[1]), np.max(gen_enc_y_result[1])))\n",
    "\n",
    "# Train\n",
    "for i in range(100001):\n",
    "\tif i < 1000:\n",
    "\t\tsess.run(learning_rate.assign(0.1))\n",
    "\telif i < 5000:\n",
    "\t\tsess.run(learning_rate.assign(0.05))\n",
    "\telse:\n",
    "\t\tsess.run(learning_rate.assign(0.02))\n",
    "\n",
    "\tbatch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\t_, gen_loss_value, gen_enc_y_result = sess.run([gen_train_step, gen_loss, gen_enc_y], feed_dict={gen_enc_x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\tsampled_z = np.random.uniform(-1.0, 1.0, size=(batch_size, encode_dims))\n",
    "\ttrain_z = np.append(sampled_z, gen_enc_y_result, axis=0)\n",
    "#\tprint gen_enc_y_result.shape\n",
    "#\tprint sampled_z.shape\n",
    "#\tprint train_z.shape\n",
    "#\tprint train_z.shape\n",
    "#\tprint ones_zeros.shape\n",
    "#\tprint zeros_ones.shape\n",
    "#\tprint disc_train_z_.shape\n",
    "#\tprint sampled_z\n",
    "#\tif i % 5 == 0:\n",
    "#\t\tplt.gcf().clear()\n",
    "#\t\tfor z_idx in range(len(train_z)):\n",
    "#\t\t\tplt.scatter(train_z[z_idx,0], train_z[z_idx,1], color='black', alpha=0.25)\n",
    "#\t\tplt.savefig('out/test_enc_' + repr(i) + '.png')\n",
    "#\t\tplt.gcf().clear()\n",
    "\n",
    "\t_, disc_disc_loss_value, disc_y_out = sess.run([disc_disc_train_step, disc_disc_loss, disc_y], feed_dict={disc_x: train_z, disc_disc_y_: disc_train_z_})\n",
    "\t_, disc_enc_loss_value = sess.run([disc_enc_train_step, disc_enc_loss], feed_dict={gen_enc_x2: batch_xs, disc_gen_y_: ones_zeros})\n",
    "\n",
    "\ty1_values.append(gen_loss_value)\n",
    "\ty2_values.append(disc_disc_loss_value)\n",
    "\ty3_values.append(disc_enc_loss_value)\n",
    "\n",
    "\tif i % 100 == 0:\n",
    "\t\tplt.gcf().clear()\n",
    "\t\tplt.plot(y1_values, '-r', label='gen_loss_value')\n",
    "\t\tplt.plot(y2_values, '-g', label='disc_disc_loss_value')\n",
    "\t\tplt.plot(y3_values, '-b', label='disc_enc_loss_value')\n",
    "\t\tplt.savefig('out/losses_' + repr(i) + '.png')\n",
    "\t\tvalidation_loss, z_out = sess.run([gen_loss, gen_enc_y], feed_dict={gen_enc_x: mnist.test.images[0:500], y_: mnist.test.labels[0:500]})\n",
    "\t\t\n",
    "\t\tdigit_colors = ['red', 'green', 'blue', 'cyan', 'magenta',\n",
    "\t\t\t'yellow', 'black', 'white', 'orange', 'gray',]\n",
    "\t\tplt.gcf().clear()\n",
    "\t\tfor z_idx in range(len(z_out)):\n",
    "\t\t\tcolor = (mnist.test.labels[z_idx] > 0).nonzero()[0][0]\n",
    "\t\t\tplt.scatter(z_out[z_idx,0], z_out[z_idx,1], color=digit_colors[color], alpha=0.25)\n",
    "\t\tplt.savefig('out/test_scatter_' + repr(i) + '.png')\n",
    "\t\tplt.gcf().clear()\n",
    "\n",
    "\t\tfor z_idx in range(batch_size):\n",
    "\t\t\tcolor = (batch_ys[z_idx] > 0).nonzero()[0][0]\n",
    "\t\t\tplt.scatter(gen_enc_y_result[z_idx,0], gen_enc_y_result[z_idx,1], color=digit_colors[color], alpha=0.25)\n",
    "\t\tplt.savefig('out/train_scatter_' + repr(i) + '.png')\n",
    "\n",
    "\t\t# Find 0-9 in the train data, encode and decode and save .jpg of decoded images\n",
    "\t\tn = 0\n",
    "\t\tidx = 0\n",
    "\t\twhile n < 10:\n",
    "\t\t\tbatch_xs = [mnist.test.images[idx]]\n",
    "\t\t\tbatch_ys = [mnist.test.labels[idx]]\n",
    "\t\t\tlogit = (mnist.test.labels[idx] > 0).nonzero()[0][0]\n",
    "\t\t\tif logit == n:\n",
    "\t\t\t\toutput_z, output_y = sess.run([gen_enc_y, gen_dec_y], feed_dict={gen_enc_x: batch_xs, y_: batch_ys})\n",
    "\t\t\t\tprint output_z\n",
    "\t\t\t\toutput_jpeg = sess.run(encode_jpeg, feed_dict={ jpeg_data: output_y })\n",
    "\t\t\t\twith open('out/test_' + repr(i) + '_' + repr(n) + '.jpg', 'w') as fd:\n",
    "\t\t\t\t\tfd.write(output_jpeg)\n",
    "\t\t\t\tn = n + 1\n",
    "\t\t\tidx = idx + 1\n",
    "\t\t# output grid of decoded digits across z\n",
    "\t\tgrid = np.array([])\n",
    "\t\tfor y in np.arange(-1.0, 1.1, 0.1):\n",
    "\t\t\tfor x in np.arange(-1.0, 1.1, 0.1):\n",
    "\t\t\t\tz = np.array([[x,y]])\n",
    "\t\t\t\toutput = sess.run([gen_dec_y2], feed_dict={gen_dec_x2: z})\n",
    "\t\t\t\toutput = np.resize(output,(28,28))\n",
    "\t\t\t\tif x == -1.0:\n",
    "\t\t\t\t\trow = output\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\trow =  np.concatenate((row, output), axis=1)\n",
    "\t\t\tif y == -1.0:\n",
    "\t\t\t\tgrid = row\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrid =  np.concatenate((grid, row), axis=0)\n",
    "\t\toutput_jpeg_grid = sess.run(encode_jpeg_grid, feed_dict={ jpeg_data_grid: grid })\n",
    "\t\twith open('out/grid_' + repr(i) + '.jpg', 'w') as fd:\n",
    "\t\t\tfd.write(output_jpeg_grid)\n",
    "\t\t# print losses\n",
    "\t\tprint('{}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}'.format(i, gen_loss_value, disc_disc_loss_value, disc_enc_loss_value, np.mean(gen_enc_y_result[0]), np.mean(gen_enc_y_result[1]), validation_loss))\n",
    "\telse:\n",
    "\t\tprint('{}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}  {:0.5f}'.format(i, gen_loss_value, disc_disc_loss_value, disc_enc_loss_value, np.mean(gen_enc_y_result[0]), np.mean(gen_enc_y_result[1])))\n",
    "\n",
    "\n",
    "\n",
    "# 0.47\n",
    "# batch 100->200\n",
    "# and reduce learning rate for disc over time\n",
    "# gives 0.42 and really good results!!! all digits recognizable in grid at 10,000 rounds, bit better at 20,000\n",
    "# batch 200->400\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
